{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "#from stats import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "#from Layer import Layer\n",
    "import tensorflow as tf \n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.datasets import fetch_california_housing, make_regression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stats(samples, features, loss, epochs, lr, batch_size, cross_k, r2, mse_train, mae_train, residual_train):\n",
    "    with open('../data/stats.csv', 'a') as f:\n",
    "        newrow = [samples, features, loss, epochs, lr, batch_size, cross_k, r2, mse_train, mae_train, residual_train]\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(newrow)\n",
    "\n",
    "def fill_dataset(dataframe: DataFrame):\n",
    "    for column in dataframe:\n",
    "        if dataframe[column].dtype != 'object':\n",
    "            dataframe[column] = dataframe[column].fillna(dataframe[column].mean())\n",
    "    return dataframe\n",
    "\n",
    "def normalize_dataset(X):\n",
    "    return tf.keras.utils.normalize(X)\n",
    "\n",
    "def remove_outliers(X, threshold=7):\n",
    "    z = np.abs(stats.zscore(X))\n",
    "    return X[(z<threshold).all(axis=1)][:, 0:-1], X[(z<threshold).all(axis=1)][: ,-1]\n",
    "\n",
    "def make_dataset(X_data,y_data,k):\n",
    "    X_data, y_data = remove_outliers(np.concatenate([X_data, y_data], axis=1))\n",
    "    def gen():\n",
    "        for train_index, test_index in KFold(k).split(X_data):\n",
    "            X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "            XN_train, XN_test = normalize_dataset(X_data[train_index]), normalize_dataset(X_data[test_index])\n",
    "            y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "            yield X_train,XN_train,y_train,X_test,XN_test,y_test\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen, (tf.double,tf.double,tf.double,tf.double,tf.double,tf.double))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>2013-02-08</td>\n",
       "      <td>67.7142</td>\n",
       "      <td>68.4014</td>\n",
       "      <td>66.8928</td>\n",
       "      <td>67.8542</td>\n",
       "      <td>158168416.0</td>\n",
       "      <td>-0.1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>2013-02-11</td>\n",
       "      <td>68.0714</td>\n",
       "      <td>69.2771</td>\n",
       "      <td>67.6071</td>\n",
       "      <td>68.5614</td>\n",
       "      <td>129029425.0</td>\n",
       "      <td>-0.4900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>2013-02-12</td>\n",
       "      <td>68.5014</td>\n",
       "      <td>68.9114</td>\n",
       "      <td>66.8205</td>\n",
       "      <td>66.8428</td>\n",
       "      <td>151829363.0</td>\n",
       "      <td>1.6586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>66.7442</td>\n",
       "      <td>67.6628</td>\n",
       "      <td>66.1742</td>\n",
       "      <td>66.7156</td>\n",
       "      <td>118721995.0</td>\n",
       "      <td>0.0286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>66.3599</td>\n",
       "      <td>67.3771</td>\n",
       "      <td>66.2885</td>\n",
       "      <td>66.6556</td>\n",
       "      <td>88809154.0</td>\n",
       "      <td>-0.2957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date     open     high      low    close       volume  target\n",
       "1259  2013-02-08  67.7142  68.4014  66.8928  67.8542  158168416.0 -0.1400\n",
       "1260  2013-02-11  68.0714  69.2771  67.6071  68.5614  129029425.0 -0.4900\n",
       "1261  2013-02-12  68.5014  68.9114  66.8205  66.8428  151829363.0  1.6586\n",
       "1262  2013-02-13  66.7442  67.6628  66.1742  66.7156  118721995.0  0.0286\n",
       "1263  2013-02-14  66.3599  67.3771  66.2885  66.6556   88809154.0 -0.2957"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \n",
    "k = 2\n",
    "epochs = 3000\n",
    "lr = 0.01\n",
    "batch_size = 120\n",
    "\n",
    "#XR, yR = make_regression(n_samples=1000, n_features=2, n_informative=5, noise=50, random_state=5)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(XR, yR, test_size=0.20, random_state=2)\n",
    "#columns = [\"{0}\".format(total_columns[i]) for i in range(XR.shape[1]+1)]\n",
    "#dataframe = pd.DataFrame(np.concatenate([XR, np.reshape(yR, [-1, 1])], axis=1), columns=columns)\n",
    "\n",
    "stocks = pd.read_csv(\"../data/datasets/all_stocks_5yr.csv\")\n",
    "dataframe = pd.DataFrame(stocks)\n",
    "filter = dataframe[\"Name\"]==\"AAPL\"\n",
    "dataframe = dataframe.where(filter).dropna()\n",
    "data = pd.DataFrame()\n",
    "\n",
    "data = dataframe.drop('Name', axis=1)\n",
    "data['target'] = dataframe['open'].fillna(dataframe['open'].mean())-dataframe['close'].fillna(dataframe['close'].mean())\n",
    "#data['type'] = 1 if dataframe['open']-dataframe['close']>=0 else 0\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  features           VIF\n",
      "0     open           inf\n",
      "1     high  5.060050e+04\n",
      "2      low  4.511802e+04\n",
      "3    close           inf\n",
      "4   volume  2.630125e+00\n",
      "5   target           inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\stats\\outliers_influence.py:195: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "C:\\Users\\Utente\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\stats\\outliers_influence.py:195: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "C:\\Users\\Utente\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\stats\\outliers_influence.py:195: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    }
   ],
   "source": [
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"features\"] = data.drop('date', axis=1).columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(data.drop('date', axis=1).values, i) for i in range(len(data.drop('date', axis=1).columns))]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(tf.Module):\n",
    "    def __init__(self, activation=tf.identity):\n",
    "        self.activation = activation\n",
    "        self.build = False\n",
    "        self.normalized = False\n",
    "        \n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def xavier_init(self, shape):\n",
    "        in_dim, out_dim = shape\n",
    "        xavier_lim = tf.sqrt(6.)/tf.sqrt(tf.cast(in_dim + out_dim, tf.float32))\n",
    "        weight_vals = tf.cast(tf.random.uniform(shape=(in_dim, out_dim), \n",
    "                                        minval=-xavier_lim, maxval=xavier_lim, seed=22, dtype=tf.float32), dtype=tf.double)\n",
    "        #xavier_lim = tf.sqrt(6.)/tf.sqrt(tf.cast(shape, tf.float32))\n",
    "        #weight_vals = tf.cast(tf.random.uniform(shape=(shape,), \n",
    "        #                                minval=-xavier_lim, maxval=xavier_lim, seed=22, dtype=tf.float32), dtype=tf.double)\n",
    "        \n",
    "        return weight_vals\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, X):\n",
    "        if not self.build:\n",
    "            n_samples, n_features = X.shape\n",
    "            self.weights = tf.Variable(self.xavier_init(shape=(n_samples, n_features)), name=\"Weights\", dtype=tf.double, trainable=True, )\n",
    "\n",
    "            self.bias = tf.Variable(tf.ones(shape=1, dtype=tf.double), name=\"Bias\", dtype=tf.double, trainable=True )\n",
    "            self.build = True\n",
    "\n",
    "        z = tf.add(tf.multiply(X, self.weights), self.bias)\n",
    "\n",
    "        #print(X.shape,\"*\",self.weights.shape,\"+\",self.bias.shape, \"=\", z.shape)\n",
    "        return self.activation(z), self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiclicAI(tf.Module):\n",
    "    def __init__(self, epochs=100, lr=0.01, batch_size=50):\n",
    "        self.epochs = epochs\n",
    "        self.batch_counter = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "        self.compiled = False\n",
    "        self.loss_history = [e for e in range(epochs)]\n",
    "        self.r2_history = [e for e in range(epochs)]\n",
    "    \n",
    "    def verify_batch(self, X):\n",
    "        if self.batch_counter >= X:\n",
    "            self.batch_counter = 0\n",
    "\n",
    "    def calc_metrics(self, e, loss, y_train, predicted_train):\n",
    "        self.loss_history[e] = loss\n",
    "        self.r2_history[e] = self.r2(y_train, predicted_train)\n",
    "\n",
    "    def r2(self, y, y_pred):\n",
    "        return tf.subtract(\n",
    "            tf.convert_to_tensor(1, dtype=tf.float64), \n",
    "            tf.divide(\n",
    "                tf.reduce_sum(tf.square(tf.subtract(y, y_pred))),\n",
    "                tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y_pred))))\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def lasso(self, weights):\n",
    "        return tf.reduce_sum(tf.norm(weights))\n",
    "\n",
    "    @tf.function\n",
    "    def ridge(self, weights):\n",
    "        return tf.reduce_sum(tf.square(tf.norm(weights)))\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def loss(self, y, predicted, weights):\n",
    "        return tf.add(tf.add(tf.losses.MSE(y, predicted), self.lasso(weights)), self.ridge(weights))\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def compile(self):\n",
    "        if not self.compiled:\n",
    "            self.layer_reg1 = Layer(tf.nn.relu)\n",
    "            self.compiled = True\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def _predict(self, X, coef, bias):\n",
    "        return tf.reduce_mean(tf.add(tf.multiply(X, coef), bias), axis=1)\n",
    "\n",
    "    def predict_test(self, X_test):\n",
    "        self.predicted_test = self._predict(X_test, self.coef, self.bias)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self._predict(x, self.coef, self.bias)\n",
    "    \n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def _forward(self, X_train):\n",
    "        return self.layer_reg1(X_train)\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def _backprop(self, weights, X_train, y_train):\n",
    "        weights, bias = self._forward(weights)\n",
    "        coef = tf.reduce_mean(tf.add(tf.matmul(tf.transpose(X_train[self.batch_counter:self.batch_counter+self.batch_size]), weights), bias), axis=0)\n",
    "        predicted = self._predict(X_train, coef, bias)\n",
    "\n",
    "        return self.loss(y_train, predicted, weights), coef, bias, predicted\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.verify_batch(X_train.shape[0])\n",
    "        weights = normalize_dataset(X_train[self.batch_counter:self.batch_counter+self.batch_size])\n",
    "        for e in range(self.epochs):\n",
    "            with tf.GradientTape(watch_accessed_variables=True, persistent=True) as tape:\n",
    "                loss, self.coef, self.bias, self.predicted_train = self._backprop(weights, X_train, y_train)\n",
    "\n",
    "            self.vars = [self.layer_reg1.weights, self.layer_reg1.bias]\n",
    "            grads = tape.gradient(loss, self.vars)  \n",
    "            self.optimizer.apply_gradients(zip(grads, self.vars))   \n",
    "            self.calc_metrics(e, loss, y_train, self.predicted_train)\n",
    "            self.batch_counter = self.batch_counter + self.batch_size\n",
    "\n",
    "            if(e%50==0):\n",
    "                print(\"[{0}] Loss: {1}, R2: {2}, Coef: {3}, Bias: {4}\".format(e, loss, self.r2_history[e], self.coef, self.bias))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23228\\1763938673.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCiclicAI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "model = CiclicAI(epochs, lr, batch_size)\n",
    "model.compile()\n",
    "model.fit(data.values, data.target)\n",
    "model.predict_test(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "ax[0].plot([e for e in range(epochs)], model.loss_history, label=\"Loss\")\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot([e for e in range(epochs)], model.r2_history, label=\"R2\")\n",
    "ax[1].set_ylabel('R2')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "fig2, ax2 = plt.subplots(2, data.values.shape[1], figsize=(20, 10))\n",
    "for i in range(data.values.shape[1]):\n",
    "    if data.values.shape[1] == 1:\n",
    "        ax2[0].scatter(data.values[:], data.target[:], cmap='viridis')\n",
    "        ax2[0].plot(data.values[:], model.predicted_train[:], color='r', zorder = 10, alpha = 0.9)\n",
    "        #ax2[0].set_ylim([-300, 300])\n",
    "        #ax2[0].set_xlim(-3,3)\n",
    "        ax2[0].set_xlabel(\"Train Vs Predicted {0}\".format(i))\n",
    "\n",
    "        ax2[1].scatter(X_test[:], y_test[:], cmap='viridis')\n",
    "        ax2[1].plot(X_test[:], model.predicted_test[:], color='r', zorder = 10, alpha = 0.9)\n",
    "        #ax2[1].set_ylim([-200, 200])\n",
    "        #ax2[1].set_xlim(-3,3)\n",
    "        ax2[1].set_xlabel(\"Test Vs Predicted {0}\".format(i))\n",
    "\n",
    "    else:\n",
    "        ax2[0, i].scatter(data.values[:, i], data.target[:], cmap='viridis')\n",
    "        ax2[0, i].plot(data.values[:, i], model.predicted_train[:], color='r', zorder = 10, alpha = 0.9)\n",
    "        #ax2[0, i].set_ylim([-300, 300])\n",
    "        #ax2[0, i].set_xlim(-3,3)\n",
    "        ax2[0, i].set_xlabel(\"Train Vs Predicted {0}\".format(i))\n",
    "\n",
    "        ax2[1, i].scatter(X_test[:, i], y_test[:], cmap='viridis')\n",
    "        ax2[1, i].plot(X_test[:, i], model.predicted_test[:], color='r', zorder = 10, alpha = 0.9)\n",
    "        #ax2[1, i].set_ylim([-200, 200])\n",
    "        #ax2[1, i].set_xlim(-3,3)\n",
    "        ax2[1, i].set_xlabel(\"Test Vs Predicted {0}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cce27c8212bf6c5aa96f33a3d1153887721b66c5c8cb9adeaa83cce09196b75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
