{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "housing = fetch_california_housing()\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.98, random_state=2)\n",
    "XR, yR = make_regression(n_samples=100, n_features=2, noise=0, random_state=5)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(XR, yR, test_size=0.20, random_state=2)\n",
    "\n",
    "def normalize_dataset(X):\n",
    "    return tf.keras.utils.normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 7.63157356e-01 -3.33903513e-01]\n",
      " [ 3.68625643e-01 -4.78704138e-01]\n",
      " [ 4.63989054e+00  2.73544567e-01]\n",
      " [ 4.37465216e-01 -6.01878426e-03]\n",
      " [-5.64168816e-02  4.81583884e-01]\n",
      " [ 2.75573790e-02  3.46293520e-01]\n",
      " [ 3.13854988e-02 -5.52448581e-02]\n",
      " [ 6.38810304e-01  8.35957976e-02]\n",
      " [ 2.26540988e-01  9.75344934e-02]\n",
      " [-1.57187509e-02  3.81434312e-02]\n",
      " [ 2.30949782e-01 -2.57217869e-01]\n",
      " [ 1.22207423e+00  8.52512608e-02]\n",
      " [ 7.75335434e-01  7.04126915e-01]\n",
      " [ 2.71752685e-01 -3.33456044e-01]\n",
      " [ 1.18625596e-01  4.76278927e-02]\n",
      " [ 3.41435758e+00  1.40816846e+00]\n",
      " [ 5.10410288e+00 -4.30880482e-01]\n",
      " [-4.98817648e-02  2.39693001e-01]\n",
      " [ 2.08505616e+00  6.59597438e-01]\n",
      " [ 7.46805778e-02 -1.33370159e-01]\n",
      " [ 1.54747280e+00  9.65034575e-02]\n",
      " [ 1.89266328e+00  2.04019924e-01]\n",
      " [ 3.83494523e-01  5.06605095e-01]\n",
      " [ 5.87800272e-01  1.85970906e+00]\n",
      " [-1.51359085e-01  1.00360555e+00]\n",
      " [ 1.50619231e+00 -1.42749798e+00]\n",
      " [ 1.12030273e-01 -1.01318596e-01]\n",
      " [ 6.84250529e-02  1.68875889e-01]\n",
      " [ 6.24384332e-01  4.53853401e-01]\n",
      " [ 1.17189859e+00  1.31129139e-01]\n",
      " [ 5.47353623e-01 -4.77137265e-01]\n",
      " [-6.62088625e-02  1.59370977e-01]\n",
      " [ 6.65831231e-02 -6.76843068e-02]\n",
      " [ 5.49488575e-01  6.43549419e-01]\n",
      " [ 2.39421011e-01  1.65786964e-01]\n",
      " [ 1.13858265e-02  7.74404334e-02]\n",
      " [ 2.59917788e-01  1.95517671e-01]\n",
      " [ 1.36237590e-01 -2.14228462e-01]\n",
      " [ 4.24291683e-01  1.00723619e+00]\n",
      " [ 1.98815359e+00  1.20743479e+00]\n",
      " [ 3.74584902e-01 -4.18054365e-01]\n",
      " [ 4.07023364e-02  4.62716061e-01]\n",
      " [ 2.68212806e-01  1.02458456e-01]\n",
      " [ 4.29120165e-02  6.30077589e-01]\n",
      " [ 8.77219934e-02 -1.34896085e-01]\n",
      " [ 5.57568020e-01  1.91535684e-01]\n",
      " [ 2.29144787e-01  8.56029412e-01]\n",
      " [ 5.40644120e-02  2.06038270e-01]\n",
      " [-7.56799508e-03  7.32397415e-02]\n",
      " [ 4.15991233e-01  5.20497658e-01]\n",
      " [ 5.27186751e+00 -1.88035316e+00]\n",
      " [ 3.75886878e-01 -2.31696954e-01]\n",
      " [ 5.30187692e-02 -1.39917362e-02]\n",
      " [-2.15327370e-01  8.30737020e-01]\n",
      " [ 2.02652421e-01  3.93679061e-01]\n",
      " [ 2.50289043e-01 -2.42630299e-01]\n",
      " [ 1.83252942e-01 -2.46632666e-01]\n",
      " [ 1.16524054e+00  8.46639577e-01]\n",
      " [ 9.63567485e-02  3.02879562e-01]\n",
      " [ 1.02485098e-01 -1.06465158e-01]\n",
      " [ 1.18154525e-02  4.59128681e-02]\n",
      " [ 5.34762884e-01  9.31333552e-01]\n",
      " [ 2.83154357e-01 -1.52964482e-01]\n",
      " [ 1.63403517e+00  2.17931933e-02]\n",
      " [ 3.22653543e-02  2.21823719e-01]\n",
      " [ 1.85020091e-01 -1.48213519e-01]\n",
      " [ 3.75643960e-01  7.98189768e-01]\n",
      " [ 9.21708047e-03 -5.14120073e-04]\n",
      " [ 2.27709923e+00  2.16455741e+00]\n",
      " [ 1.13830575e-01 -2.60198114e-02]\n",
      " [ 4.51200000e+00 -6.34433781e-01]\n",
      " [ 1.62669188e-02  1.45303584e-01]\n",
      " [ 1.69016224e+00  7.93660585e-01]\n",
      " [ 6.42316694e-01  1.98075115e-01]\n",
      " [ 2.64109167e+00  9.62268352e-01]\n",
      " [-6.81049713e-02  1.46767333e-01]\n",
      " [ 5.29791575e-01  4.41770898e-01]\n",
      " [ 1.54146751e+00  2.87357642e+00]\n",
      " [ 5.03176538e-02 -9.13970426e-03]\n",
      " [ 4.65224032e+00  1.20942209e-01]], shape=(80, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "class Layer(tf.Module):\n",
    "    def __init__(self, activation=tf.identity):\n",
    "        self.activation = activation\n",
    "        self.build = False\n",
    "        self.normalized = False\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def xavier_init(self, shape):\n",
    "        in_dim, out_dim = shape\n",
    "        xavier_lim = tf.sqrt(6.)/tf.sqrt(tf.cast(in_dim + out_dim, tf.float32))\n",
    "        weight_vals = tf.cast(tf.random.uniform(shape=(in_dim, out_dim), \n",
    "                                        minval=-xavier_lim, maxval=xavier_lim, seed=22, dtype=tf.float32), dtype=tf.double)\n",
    "        return weight_vals\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, X):\n",
    "        if not self.build:\n",
    "            n_samples, n_features = X.shape\n",
    "            self.weights = tf.Variable(self.xavier_init(shape=(n_features, n_samples)), name=\"Weights\", dtype=tf.double, trainable=True, )\n",
    "            self.bias = tf.Variable(tf.zeros(shape=1, dtype=tf.double), name=\"Bias\", dtype=tf.double, trainable=True )\n",
    "            self.build = True\n",
    "\n",
    "        z = tf.add(tf.matmul(X, self.weights), self.bias)\n",
    "\n",
    "        print(X.shape,\"*\",self.weights.shape,\"+\",self.bias.shape, \"=\", z.shape)\n",
    "        return self.activation(z)\n",
    "\n",
    "    \n",
    "\n",
    "class MLPLinearRegressor(tf.Module):\n",
    "    def __init__(self, layers,XR, X, XN, X_test, y, y_test, lr=0.001, epochs=10):\n",
    "        self.layers = layers\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.loaded = False\n",
    "        self.X = X \n",
    "        self.dataset = XR\n",
    "        self.XN = XN\n",
    "        self.XN_test = normalize_dataset(X_test)\n",
    "        self.y = y\n",
    "        self.y_test = y_test\n",
    "        self.X_test = X_test\n",
    "        self.learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=1000000, decay_rate=0.96, staircase=True)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "        self.train_mse = tf.keras.metrics.MeanSquaredError()\n",
    "        self.train_mae = tf.keras.metrics.MeanAbsoluteError()\n",
    "        self.train_accuracy = tf.keras.metrics.MeanSquaredLogarithmicError()\n",
    "        self.test_mse = tf.keras.metrics.MeanSquaredError()\n",
    "        self.test_mae = tf.keras.metrics.MeanAbsoluteError()\n",
    "        self.test_accuracy = tf.keras.metrics.MeanSquaredLogarithmicError()\n",
    "        self.regularizer = tf.keras.layers.ActivityRegularization()\n",
    "        self.loss_history = [e for e in range(epochs)]\n",
    "        self.bias_history = [e for e in range(epochs)]\n",
    "        self.mae_train_error_history = [e for e in range(epochs)]\n",
    "        self.mse_train_error_history = [e for e in range(epochs)]\n",
    "        self.mae_test_error_history = [e for e in range(epochs)]\n",
    "        self.mse_test_error_history = [e for e in range(epochs)]\n",
    "        self.predicted_train_history = [[ i for i in range(self.y.shape[0]) ] for e in range(epochs)]\n",
    "        self.regloss_history = [e for e in range(epochs)]\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def train_accuracy_rrsse(self, predicted):\n",
    "        return tf.divide(tf.sqrt(\n",
    "            tf.divide(\n",
    "                tf.reduce_sum(tf.square(tf.abs((tf.subtract(predicted, self.y))))),\n",
    "                tf.reduce_sum(tf.square(tf.abs((tf.subtract(tf.reduce_mean(self.y), predicted)))))\n",
    "            )), predicted.shape[0]\n",
    "        )\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def calc_metrics(self):\n",
    "        self.train_mse.reset_state()\n",
    "        self.train_mae.reset_state()\n",
    "        self.test_mse.reset_state()\n",
    "        self.test_mae.reset_state()\n",
    "        self.train_accuracy.update_state(self.y, self.predicted)\n",
    "        self.train_mse.update_state(self.y, self.predicted)\n",
    "        self.train_mae.update_state(self.y, self.predicted)\n",
    "        self.test_accuracy.update_state(self.y_test, self.predicted_test)\n",
    "        self.test_mse.update_state(self.y_test, self.predicted_test)\n",
    "        self.test_mae.update_state(self.y_test, self.predicted_test)\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def train_step(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def _predict_train(self, X, weights):\n",
    "        compressed_weights = tf.reduce_mean(tf.subtract(tf.multiply(X, weights), self.bias), axis=1)\n",
    "        return tf.reduce_mean(tf.add(tf.multiply(tf.transpose(X), compressed_weights), self.bias), axis=1)\n",
    "\n",
    "    def predict_test(self):\n",
    "        self.predicted_test = tf.reduce_mean(tf.add(tf.multiply(self.X_test, self.compressed_weights), self.bias), axis=1)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return tf.reduce_mean(tf.add(tf.multiply(x, self.compressed_weights), self.bias))\n",
    "\n",
    "    def calc_history(self, e, loss):\n",
    "        self.train_mae.update_state(self.y, self.predicted)\n",
    "        self.train_mse.update_state(self.y, self.predicted)\n",
    "        self.test_mae.update_state(self.y_test, self.predicted_test)\n",
    "        self.test_mse.update_state(self.y_test, self.predicted_test)\n",
    "        self.bias_history[e] = self.bias\n",
    "        self.loss_history[e] = loss.numpy()\n",
    "        self.predicted_train_history[e] = self.predicted\n",
    "        self.mae_train_error_history[e] = self.train_mae.result().numpy()\n",
    "        self.mse_train_error_history[e] = self.train_mse.result().numpy()\n",
    "        self.mae_test_error_history[e] = self.test_mae.result().numpy()\n",
    "        self.mse_test_error_history[e] = self.test_mse.result().numpy()\n",
    "    \n",
    "    @tf.function\n",
    "    def lasso(self):\n",
    "        lassos = [w for w in range(len(self.layers))]\n",
    "        for w in range(len(self.layers)):\n",
    "            lassos[w] = tf.reduce_sum(tf.norm(self.layers[w].weights))\n",
    "        return tf.reduce_mean(lassos)\n",
    "\n",
    "    @tf.function\n",
    "    def ridge(self):\n",
    "        ridges = [w for w in range(len(self.layers))]\n",
    "        for w in range(len(self.layers)):\n",
    "            ridges[w] = tf.reduce_sum(tf.square(tf.norm(self.layers[w].weights)))\n",
    "        return tf.reduce_mean(ridges)\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def loss(self, predicted):\n",
    "        return tf.add(tf.losses.MSE(self.y, predicted), self.lasso())\n",
    "\n",
    "    def run(self):\n",
    "        for i, j in enumerate(self.XN.split(self.dataset)):\n",
    "            self.weights = model.train_step(self.dataset[i])\n",
    "\n",
    "        self.vars = [self.layers[0].weights, self.layers[1].weights, self.layers[2].weights,self.layers[3].weights,self.layers[4].weights, self.layers[0].bias, self.layers[1].bias, self.layers[2].bias,self.layers[3].bias,self.layers[4].bias]\n",
    "        self.weights_list = [self.layers[0].weights, self.layers[1].weights, self.layers[2].weights,self.layers[3].weights,self.layers[4].weights,]\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            with tf.GradientTape(watch_accessed_variables=True, persistent=True) as tape:\n",
    "                for i, train_index in enumerate(self.XN.split(self.dataset)):\n",
    "                    print(i, train_index)\n",
    "                    self.weights = self.train_step(self.XN)\n",
    "                    self.bias = self.layers[len(self.layers)-1].bias\n",
    "                    self.flatten_weights = tf.reduce_mean(self.weights, axis=0)\n",
    "                    self.predicted = self._predict_train(tf.transpose(self.X), self.flatten_weights)\n",
    "                    \n",
    "                    loss = tf.nn.scale_regularization_loss(self.loss(self.predicted)) \n",
    "\n",
    "                    self.compressed_weights = tf.reduce_mean(tf.add(tf.multiply(tf.transpose(self.XN), tf.reduce_mean(self.weights, axis=0)), self.bias), axis=1)\n",
    "                    self.predict_test()\n",
    "\n",
    "                    self.calc_history(e, loss)\n",
    "\n",
    "            grads = tape.gradient(loss, self.vars)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.vars)) \n",
    "\n",
    "model = MLPLinearRegressor([\n",
    "    Layer(),\n",
    "    Layer(),\n",
    "    Layer(activation=tf.nn.relu),\n",
    "    Layer(activation=tf.sigmoid),\n",
    "    Layer()\n",
    "],XR, Xk, XNk, X_test2, tf.squeeze(y_train2), tf.squeeze(y_test2), lr=0.01, epochs=120)\n",
    "\n",
    "\n",
    "model.run()\n",
    "model.calc_metrics()\n",
    "print(\"Original: \", y_test2[0], \"...Predicted: \", model.predict(X_test2[0]).numpy(), \"...Tested:\", model.predicted_test[0].numpy())\n",
    "\n",
    "print(\"Train_MeanSquaredError: \", model.train_mse.result().numpy())\n",
    "print(\"Train_MeanAbsoluteError: \", model.train_mae.result().numpy())\n",
    "print(\"Train_MeanSquaredLogError: \", model.train_accuracy.result().numpy())\n",
    "print('--------------------------------------------------------')\n",
    "print(\"Test_MeanSquaredError: \", model.test_mse.result().numpy())\n",
    "print(\"Train_MeanAbsoluteError: \", model.test_mae.result().numpy())\n",
    "print(\"Test_MeanSquaredLogError: \", model.test_accuracy.result().numpy())\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "ax[0].plot([e for e in range(model.epochs)], model.loss_history, label=\"Loss\")\n",
    "ax[0].plot([e for e in range(model.epochs)], tf.square(model.bias_history).numpy(), label=\"Bias\")\n",
    "ax[0].set_ylabel('Loss & Bias History')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot([e for e in range(model.epochs)], model.mae_train_error_history, label=\"Train MAE\")\n",
    "ax[1].plot([e for e in range(model.epochs)], model.mae_test_error_history, label=\"Test MAE\")\n",
    "ax[1].plot([e for e in range(model.epochs)], model.mse_train_error_history, label=\"Train MSE\")\n",
    "ax[1].plot([e for e in range(model.epochs)], model.mse_test_error_history, label=\"Test MSE\")\n",
    "ax[1].set_ylabel('Metrics History')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].scatter(X_train2[:, -1], y_train2[:])\n",
    "ax[2].plot(X_train2[:, -1], model.predicted[:], color='r')\n",
    "#ax[2].set_ylim([-200, 200])\n",
    "#ax[2].set_xlim(-3,3)\n",
    "ax[2].set_xlabel(\"Train\")\n",
    "\n",
    "ax[3].scatter(X_test2[:, -1], y_test2[:])\n",
    "ax[3].plot(X_test2[:, -1], model.predicted_test[:], color='r')\n",
    "#ax[3].set_ylim([-200, 200])\n",
    "#ax[3].set_xlim(-3,3)\n",
    "ax[3].set_xlabel(\"Test\")\n",
    "\n",
    "\"\"\"fig3d = plt.figure()\n",
    "ax3d = fig3d.add_subplot(projection='3d')\n",
    "ax3d.scatter(X_train2[:, 0], X_train2[:, 1], y_train2[:])\n",
    "ax3d.plot(X_train2[:, 0], X_train2[:, 1], model.predicted[:])\n",
    "\"\"\"\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cce27c8212bf6c5aa96f33a3d1153887721b66c5c8cb9adeaa83cce09196b75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
