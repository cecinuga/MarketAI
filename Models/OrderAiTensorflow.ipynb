{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "housing = fetch_california_housing()\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.98, random_state=2)\n",
    "XR, yR = make_regression(n_samples=100, n_features=2, noise=0, random_state=5)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(XR, yR, test_size=0.20, random_state=2)\n",
    "\n",
    "def normalize_dataset(X):\n",
    "    return tf.keras.utils.normalize(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 2) * (2, 10) + (1,) = (80, 10)\n",
      "(80, 2) * (2, 10) + (1,) = (80, 10)\n",
      "(80, 10) * (10, 10) + (1,) = (80, 10)\n",
      "(80, 10) * (10, 10) + (1,) = (80, 10)\n",
      "(80, 10) * (10, 10) + (1,) = (80, 10)\n",
      "(80, 10) * (10, 10) + (1,) = (80, 10)\n",
      "(80, 10) * (10, 10) + (1,) = (80, 10)\n",
      "(80, 10) * (10, 10) + (1,) = (80, 10)\n",
      "(80, 10) * (10, 1) + (1,) = (80, 1)\n",
      "(80, 10) * (10, 1) + (1,) = (80, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLPLinearRegressor' object has no attribute 'predicted'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[659], line 104\u001b[0m\n\u001b[0;32m     96\u001b[0m model \u001b[39m=\u001b[39m MLPLinearRegressor([\n\u001b[0;32m     97\u001b[0m     Layer(neurons\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, activation\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39msigmoid,),\n\u001b[0;32m     98\u001b[0m     Layer(neurons\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, activation\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39msigmoid,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     Layer(neurons\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39msigmoid,)\n\u001b[0;32m    102\u001b[0m ])\n\u001b[0;32m    103\u001b[0m model\u001b[39m.\u001b[39mload_data(normalize_dataset(X_train2), tf\u001b[39m.\u001b[39msqueeze(y_train2))\n\u001b[1;32m--> 104\u001b[0m model\u001b[39m.\u001b[39;49mtrain(epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m    105\u001b[0m model\u001b[39m.\u001b[39mpredict()\n\u001b[0;32m    106\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mpredicted)\n",
      "Cell \u001b[1;32mIn[659], line 88\u001b[0m, in \u001b[0;36mMLPLinearRegressor.train\u001b[1;34m(self, epochs, lr)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvars))\n\u001b[0;32m     87\u001b[0m     \u001b[39m#self.predicted = self.train_step(self.X)\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m model\u001b[39m.\u001b[39mcalc_metrics(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredicted)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MLPLinearRegressor' object has no attribute 'predicted'"
     ]
    }
   ],
   "source": [
    "class Layer(tf.Module):\n",
    "    def __init__(self, neurons=30, activation=tf.identity):\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.build = False\n",
    "        self.normalized = False\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def xavier_init(self, shape):\n",
    "        in_dim, out_dim = shape\n",
    "        xavier_lim = tf.sqrt(6.)/tf.sqrt(tf.cast(in_dim + out_dim, tf.float32))\n",
    "        weight_vals = tf.cast(tf.random.uniform(shape=(in_dim, out_dim), \n",
    "                                        minval=-xavier_lim, maxval=xavier_lim, seed=22, dtype=tf.float32), dtype=tf.double)\n",
    "        return weight_vals\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, X):\n",
    "        \"\"\"if self.index == 1:\n",
    "            z = tf.add(tf.matmul(X, self.weights), self.bias)\n",
    "            print(z)\n",
    "        elif self.index > 1:\n",
    "            z = tf.add(tf.matmul(X, tf.transpose(self.weights)), self.bias)\n",
    "        elif self.Last:\n",
    "            z = tf.add(tf.multiply(X, tf.reduce_mean(self.weights, axis=0)), self.bias)\n",
    "            return self.activation(tf.reduce_mean(z, axis=1))\"\"\"\n",
    "\n",
    "        if not self.build:\n",
    "            n_features = X.shape[1]\n",
    "            self.weights = tf.Variable(self.xavier_init(shape=(n_features, self.neurons)), name=\"Weights\", dtype=tf.double, trainable=True )\n",
    "            self.bias = tf.Variable(tf.zeros(shape=1, dtype=tf.double), name=\"Bias\", dtype=tf.double, trainable=True )\n",
    "            self.build = True\n",
    "\n",
    "        z = tf.add(tf.matmul(X, self.weights), self.bias)\n",
    "        print(X.shape,\"*\",self.weights.shape,\"+\",self.bias.shape, \"=\", z.shape)\n",
    "        return self.activation(z)\n",
    "\n",
    "    \n",
    "\n",
    "class MLPLinearRegressor(tf.Module):\n",
    "    def __init__(self, layers, lr=0.001):\n",
    "        self.layers = layers\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr)\n",
    "        self.mse = tf.keras.metrics.MeanSquaredError()\n",
    "        self.accuracy = tf.keras.metrics.MeanSquaredLogarithmicError()\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "\n",
    "    def load_data(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def accuracy_rrsse(self, predicted):\n",
    "        return tf.divide(tf.sqrt(\n",
    "            tf.divide(\n",
    "                tf.reduce_sum(tf.square(tf.abs((tf.subtract(tf.squeeze(predicted), tf.sigmoid(self.y)))))),\n",
    "                tf.reduce_sum(tf.square(tf.abs((tf.subtract(tf.reduce_mean(tf.sigmoid(self.y)), tf.squeeze(predicted))))))\n",
    "            )), len(predicted)\n",
    "        )\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def loss(self, predicted):\n",
    "        #return 1/2 * tf.reduce_sum( tf.square(tf.subtract(tf.sigmoid(self.y), tf.squeeze(predicted))))\n",
    "        return tf.losses.mean_squared_error(tf.sigmoid(self.y), tf.squeeze(predicted))\n",
    "\n",
    "    @tf.function\n",
    "    def calc_metrics(self, y_pred):\n",
    "        self.acc_rrsse = self.accuracy_rrsse(y_pred)\n",
    "        self.accuracy.update_state(tf.sigmoid(self.y), tf.squeeze(y_pred))\n",
    "        self.precision.update_state(tf.sigmoid(self.y), tf.squeeze(y_pred))\n",
    "        self.mse.update_state(tf.sigmoid(self.y), tf.squeeze(y_pred))\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def train(self, epochs=1, lr=0.001):\n",
    "        for e in range(epochs):\n",
    "            with tf.GradientTape(watch_accessed_variables=True, persistent=True) as tape:\n",
    "                self.y_pred = model.train_step(self.X)\n",
    "                loss = self.loss(self.y_pred)\n",
    "            self.vars = [self.layers[0].weights, self.layers[1].weights, self.layers[2].weights,self.layers[3].weights,self.layers[4].weights, self.layers[0].bias, self.layers[1].bias, self.layers[2].bias,self.layers[3].bias,self.layers[4].bias]\n",
    "            grads = tape.gradient(loss, self.vars) \n",
    "            self.optimizer.apply_gradients(zip(grads, self.vars))\n",
    "            #self.predicted = self.train_step(self.X)\n",
    "        model.calc_metrics(self.predicted)\n",
    "    \n",
    "    @tf.function\n",
    "    def predict(self):\n",
    "        self.predicted = tf.add(tf.matmul(tf.transpose(self.X), self.y_pred, ), self.layers[-1].bias)\n",
    "\n",
    "\n",
    "\n",
    "model = MLPLinearRegressor([\n",
    "    Layer(neurons=10, activation=tf.sigmoid,),\n",
    "    Layer(neurons=10, activation=tf.sigmoid,),\n",
    "    Layer(neurons=10, activation=tf.sigmoid,),\n",
    "    Layer(neurons=10, activation=tf.sigmoid,),\n",
    "    Layer(neurons=1, activation=tf.sigmoid,)\n",
    "])\n",
    "model.load_data(normalize_dataset(X_train2), tf.squeeze(y_train2))\n",
    "model.train(epochs=100, lr=0.01)\n",
    "model.predict()\n",
    "print(model.predicted)\n",
    "print(\"MeanSquaredError: \", model.mse.result().numpy())\n",
    "print(\"Keras Accuracy: \", model.accuracy.result().numpy())\n",
    "print(\"Keras Precision: \", model.precision.result().numpy())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(X_train2[:, 0], y_train2[:])\n",
    "#ax.scatter(np.linspace(-3, 3, num=model.predicted.shape[0]), tf.multiply(X_train2, tf.reduce_mean(model.layers[4].weights, axis=1))[:, 0])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cce27c8212bf6c5aa96f33a3d1153887721b66c5c8cb9adeaa83cce09196b75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
