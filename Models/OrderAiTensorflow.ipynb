{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001EF06EC5CA0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):  File \"c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)  File \"C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_18940\\2333311792.py\", line 58, in <module>\n",
      "    model.initialize(X_train, y_train)  File \"C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_18940\\2333311792.py\", line 31, in initialize\n",
      "    self.activation.write(l, [tf.math.sigmoid(tf.matmul(self.activation.read(l-1)[n], self.h_weights[l][n]) + self.bias[l]) for n in range(self.n_neurons)])  File \"C:\\Users\\Utente\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 243, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001EF06E4F7F0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):  File \"c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)  File \"C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_18940\\2333311792.py\", line 58, in <module>\n",
      "    model.initialize(X_train, y_train)  File \"C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_18940\\2333311792.py\", line 32, in initialize\n",
      "    self.activation.write(self.n_layers-1, [tf.math.sigmoid(tf.matmul(self.activation.read(self.n_layers-2)[n], self.h_weights[self.n_layers-1][n]) + self.bias[self.n_layers-1]) for n in range(self.n_neurons)])  File \"C:\\Users\\Utente\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 243, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['f_weights:0', 'h_weights:0', 'bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'f_weights:0' shape=(3, 412, 8) dtype=float64>), (None, <tf.Variable 'h_weights:0' shape=(3, 3, 412, 412) dtype=float64>), (None, <tf.Variable 'bias:0' shape=(3,) dtype=float64>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(housing\u001b[39m.\u001b[39mdata, housing\u001b[39m.\u001b[39mtarget, test_size\u001b[39m=\u001b[39m\u001b[39m0.98\u001b[39m)\n\u001b[0;32m     57\u001b[0m model \u001b[39m=\u001b[39m MLPLinearRegressor()\n\u001b[1;32m---> 58\u001b[0m model\u001b[39m.\u001b[39;49minitialize(X_train, y_train)\n\u001b[0;32m     59\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[27], line 42\u001b[0m, in \u001b[0;36mMLPLinearRegressor.initialize\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mschedules\u001b[39m.\u001b[39mExponentialDecay(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr, \u001b[39m5\u001b[39m, \u001b[39m0.85\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLearning_Rate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[39m#self.optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate).minimize(self.loss, var_list=[self.f_weights, self.h_weights, self.bias], name=\"Optimizer\")\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49moptimizers\u001b[39m.\u001b[39;49mAdam(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_rate)\u001b[39m.\u001b[39;49mminimize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss, var_list\u001b[39m=\u001b[39;49m[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mh_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias], name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mOptimizer\u001b[39;49m\u001b[39m\"\u001b[39;49m, tape\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mGradientTape())\n\u001b[0;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorrects \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mequal(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicted, y)\n\u001b[0;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccuracy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39mcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorrects, tf\u001b[39m.\u001b[39mfloat32))\n",
      "File \u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:579\u001b[0m, in \u001b[0;36mOptimizerV2.minimize\u001b[1;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[39mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    574\u001b[0m \n\u001b[0;32m    575\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    576\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_gradients(\n\u001b[0;32m    577\u001b[0m     loss, var_list\u001b[39m=\u001b[39mvar_list, grad_loss\u001b[39m=\u001b[39mgrad_loss, tape\u001b[39m=\u001b[39mtape\n\u001b[0;32m    578\u001b[0m )\n\u001b[1;32m--> 579\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:689\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_gradients\u001b[39m(\n\u001b[0;32m    649\u001b[0m     \u001b[39mself\u001b[39m, grads_and_vars, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, experimental_aggregate_gradients\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    650\u001b[0m ):\n\u001b[0;32m    651\u001b[0m     \u001b[39m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \n\u001b[0;32m    653\u001b[0m \u001b[39m    This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39m      RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m optimizer_utils\u001b[39m.\u001b[39;49mfilter_empty_gradients(grads_and_vars)\n\u001b[0;32m    690\u001b[0m     var_list \u001b[39m=\u001b[39m [v \u001b[39mfor\u001b[39;00m (_, v) \u001b[39min\u001b[39;00m grads_and_vars]\n\u001b[0;32m    692\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name):\n\u001b[0;32m    693\u001b[0m         \u001b[39m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\utils.py:77\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filtered:\n\u001b[0;32m     76\u001b[0m     variable \u001b[39m=\u001b[39m ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable: \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided `grads_and_vars` is \u001b[39m\u001b[39m{\u001b[39;00mgrads_and_vars\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m     )\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m vars_with_empty_grads:\n\u001b[0;32m     82\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m     83\u001b[0m         (\n\u001b[0;32m     84\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mGradients do not exist for variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m when minimizing the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m         ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vars_with_empty_grads]),\n\u001b[0;32m     89\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: (['f_weights:0', 'h_weights:0', 'bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'f_weights:0' shape=(3, 412, 8) dtype=float64>), (None, <tf.Variable 'h_weights:0' shape=(3, 3, 412, 412) dtype=float64>), (None, <tf.Variable 'bias:0' shape=(3,) dtype=float64>))."
     ]
    }
   ],
   "source": [
    "class MLPLinearRegressor(object):\n",
    "    def __init__(self, lr=0.001, n_layers=3, n_neurons=3):\n",
    "        self.graph = tf.Graph()\n",
    "        self.lr = 0.001\n",
    "        self.n_layers = n_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.initialized = False\n",
    "\n",
    "    @tf.function\n",
    "    def loss(self):\n",
    "        return 1/2 * tf.reduce_sum( tf.square((tf.reshape(self.predicted, [-1, 1]) - self.y)))\n",
    "\n",
    "\n",
    "    def initialize(self, X, y):\n",
    "        with self.graph.as_default():            \n",
    "            n_samples, n_features = X.shape\n",
    "            n_output = y.shape[0]\n",
    "            self.activation = {}\n",
    "            normal_initiliazer = tf.random_normal_initializer(seed=0, mean=0.0, stddev=1.0)\n",
    "            XN = tf.keras.utils.normalize(X)\n",
    "            self.X = tf.constant(XN, name=\"X\", dtype=tf.double)\n",
    "            self.y = tf.constant(np.array(y).reshape(-1, 1), name=\"y\", dtype=tf.double)\n",
    "\n",
    "            self.f_weights = tf.Variable(normal_initiliazer([self.n_neurons, n_samples, n_features], dtype=tf.double), name=\"f_weights\", dtype=tf.double, trainable=True )\n",
    "            self.h_weights = tf.Variable(normal_initiliazer([self.n_layers, self.n_neurons, n_samples, n_output], dtype=tf.double), name=\"h_weights\", dtype=tf.double, trainable=True )\n",
    "            self.bias = tf.Variable([0.01 for l in range(self.n_layers)], name=\"bias\", dtype=tf.double, trainable=True )\n",
    "            \n",
    "            self.activation = tf.TensorArray(tf.double, size=0, dynamic_size=True, clear_after_read=False, tensor_array_name=\"Activation_Layers\")\n",
    "            self.activation.write(0, [tf.math.sigmoid(tf.matmul(X, tf.transpose(self.f_weights[n])) + self.bias[0]) for n in range(self.n_neurons)]).mark_used()\n",
    "            for l in range(1,self.n_layers-1):\n",
    "                self.activation.write(l, [tf.math.sigmoid(tf.matmul(self.activation.read(l-1)[n], self.h_weights[l][n]) + self.bias[l]) for n in range(self.n_neurons)])\n",
    "            self.activation.write(self.n_layers-1, [tf.math.sigmoid(tf.matmul(self.activation.read(self.n_layers-2)[n], self.h_weights[self.n_layers-1][n]) + self.bias[self.n_layers-1]) for n in range(self.n_neurons)])\n",
    "\n",
    "\n",
    "            mean_weights = tf.reduce_mean(self.h_weights[self.n_layers-1], axis=0)\n",
    "            mean_activation = tf.reduce_mean(self.activation.read(self.n_layers-1), axis=1)\n",
    "\n",
    "            self.predicted = tf.reduce_mean(tf.math.sigmoid(tf.matmul(mean_activation, mean_weights) + tf.reshape(self.bias[self.n_layers-1], [-1, 1])), axis=0)\n",
    "            #self.loss = lambda : tf.losses.mean_squared_error(self.predicted, y)\n",
    "            self.learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(self.lr, 5, 0.85, name=\"Learning_Rate\")\n",
    "            #self.optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate).minimize(self.loss, var_list=[self.f_weights, self.h_weights, self.bias], name=\"Optimizer\")\n",
    "            self.optimizer = tf.optimizers.Adam(self.learning_rate).minimize(self.loss, var_list=[self.f_weights, self.h_weights, self.bias], name=\"Optimizer\")\n",
    "\n",
    "            self.corrects = tf.equal(self.predicted, y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.corrects, tf.float32))\n",
    "\n",
    "            self.initialized = True\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        if self.initialized:\n",
    "            pass\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.98)\n",
    "\n",
    "model = MLPLinearRegressor()\n",
    "model.initialize(X_train, y_train)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cce27c8212bf6c5aa96f33a3d1153887721b66c5c8cb9adeaa83cce09196b75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
