{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001EF00F15E80>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):  File \"c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)  File \"C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_18940\\750269629.py\", line 54, in <module>\n",
      "    model.initialize(X_train, y_train)  File \"C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_18940\\750269629.py\", line 28, in initialize\n",
      "    self.activation.write(l, [tf.math.sigmoid(tf.matmul(self.activation.read(l-1)[n], self.h_weights[l][n]) + self.bias[l]) for n in range(self.n_neurons)])  File \"C:\\Users\\Utente\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 243, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001EF00F15E80>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):  File \"c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)  File \"C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_18940\\750269629.py\", line 54, in <module>\n",
      "    model.initialize(X_train, y_train)  File \"C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_18940\\750269629.py\", line 29, in initialize\n",
      "    self.activation.write(self.n_layers-1, [tf.math.sigmoid(tf.matmul(self.activation.read(self.n_layers-2)[n], self.h_weights[self.n_layers-1][n]) + self.bias[self.n_layers-1]) for n in range(self.n_neurons)])  File \"C:\\Users\\Utente\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 243, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['f_weights:0', 'h_weights:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'f_weights:0' shape=(3, 1032, 8) dtype=float64, numpy=\narray([[[ 0.50597862, -0.70111141,  0.45309454, ...,  0.57622276,\n         -0.16532128,  0.14021994],\n        [-1.33475533, -0.07734598,  1.43766396, ..., -0.0521582 ,\n          1.07995859, -0.88625755],\n        [-1.02305545,  1.54611159,  1.28781231, ...,  0.14079827,\n         -0.33146609,  1.59719274],\n        ...,\n        [ 0.73296047,  0.0603372 , -2.22900904, ...,  0.34560146,\n         -0.20135526,  0.16414816],\n        [ 1.53523747, -0.54952954, -1.4167278 , ..., -0.38099626,\n          1.34786757, -1.55258931],\n        [ 3.05293555,  0.89496054,  1.0255621 , ...,  1.33858007,\n          0.10878851,  1.1138124 ]],\n\n       [[ 1.72520435, -0.02254615,  0.4565958 , ...,  0.25351815,\n         -1.49348338, -0.02426565],\n        [-1.73683138, -3.13296541, -1.0009193 , ..., -0.6623713 ,\n         -1.55629119, -1.27437937],\n        [-0.52004223, -0.65473797,  0.1437589 , ..., -0.65674196,\n          0.26696839, -0.36011948],\n        ...,\n        [-1.17803299,  0.7581498 ,  2.57487294, ..., -1.42498255,\n         -1.35024898,  0.2043445 ],\n        [-1.92464358,  0.75313672,  0.91514881, ..., -1.02062905,\n          0.06891263,  0.02138806],\n        [-3.33474558,  2.78310279,  0.78618056, ..., -0.4866121 ,\n          0.42650806,  1.65580502]],\n\n       [[ 0.45416331, -1.62202166, -1.0840212 , ...,  0.94933645,\n          0.0423798 ,  0.20728952],\n        [-0.80985414,  1.75969839, -0.63845318, ..., -0.27194673,\n         -1.12112559,  0.13007384],\n        [-2.38110683, -0.11047422, -1.45644089, ..., -0.27660387,\n         -1.38421801,  1.20912123],\n        ...,\n        [ 0.12850507, -0.3466529 , -0.93076991, ...,  0.5410156 ,\n         -0.96472201, -0.41431831],\n        [-0.02140139, -0.96099732, -0.17851672, ..., -0.4628775 ,\n          0.83304263,  0.12199759],\n        [ 0.23580939, -1.34881401,  0.40596882, ...,  0.03102438,\n         -1.61135956,  1.3708435 ]]])>), (None, <tf.Variable 'h_weights:0' shape=(3, 3, 1032, 1032) dtype=float64, numpy=\narray([[[[ 5.05978617e-01, -7.01111411e-01,  4.53094537e-01, ...,\n           2.01663704e-01, -6.87309775e-01,  1.12341927e+00],\n         [-3.94356741e-01,  3.97808510e-01, -1.41928089e+00, ...,\n           3.19940172e-01, -4.66404894e-01, -3.62271970e-01],\n         [ 4.53668614e-02,  1.07113242e+00,  1.06879203e+00, ...,\n           1.26675523e-01,  1.20327052e+00,  1.67265897e+00],\n         ...,\n         [-1.78911155e+00,  1.36082847e+00, -1.88817905e-01, ...,\n          -1.95171538e+00, -7.79169681e-02,  3.33521615e-01],\n         [-3.03668185e-01,  3.72255576e-01, -5.04441293e-01, ...,\n           4.10655743e-01,  1.02266630e+00, -3.36224293e-01],\n         [-7.19580875e-01, -2.45929627e-02, -4.61183365e-01, ...,\n          -1.35409845e+00,  1.11598508e+00, -1.08919482e+00]],\n\n        [[-9.67877317e-01,  2.52314536e-01,  3.42320441e-01, ...,\n           1.07742695e+00, -7.68895277e-02,  1.79941101e+00],\n         [ 4.94328881e-01,  5.12484565e-03,  9.10818098e-01, ...,\n          -1.45903754e+00, -1.67448395e+00,  1.75479179e+00],\n         [ 1.21336286e+00, -1.72999873e-01, -1.16477214e+00, ...,\n           1.85513232e+00, -1.79540337e-03, -3.81298381e-01],\n         ...,\n         [-1.36683062e-01,  1.74906848e+00, -7.95266196e-01, ...,\n          -7.73560432e-01, -2.06929636e-01, -2.30326988e-01],\n         [-6.06685361e-01,  5.04122289e-01, -3.40230956e-01, ...,\n           1.68160856e+00, -9.37543630e-01, -8.85798498e-01],\n         [-1.62052470e-02, -9.48159899e-01,  1.85482769e+00, ...,\n          -2.69687453e-02, -5.77886718e-01,  9.75548857e-01]],\n\n        [[ 7.41917662e-02,  8.07308483e-01, -1.32015877e-01, ...,\n          -2.63321551e+00, -1.55754927e+00, -1.12279820e+00],\n         [ 1.74868012e+00, -1.57442408e+00, -4.05335610e-01, ...,\n           7.37842015e-01,  9.73400726e-01, -6.07860281e-02],\n         [ 1.00761411e-01,  9.56840226e-01,  9.78929271e-01, ...,\n          -7.86378347e-01,  2.35566413e-01,  4.85067309e-01],\n         ...,\n         [ 7.29662501e-01,  1.04465823e-01, -5.17342432e-01, ...,\n           4.68843748e-01, -2.02585139e+00, -8.50086319e-01],\n         [ 1.00846286e+00, -8.70257417e-01,  6.53197863e-01, ...,\n          -5.42855965e-01, -6.79427592e-01, -3.28299837e-01],\n         [ 1.73219711e-01, -1.95214891e+00,  8.06227223e-01, ...,\n           3.31526358e-01,  4.82596937e-01, -8.88833590e-01]]],\n\n\n       [[[ 3.60229238e-01,  3.10395345e+00,  4.89947923e-01, ...,\n           1.04423519e-01,  2.64872555e+00,  1.48999485e+00],\n         [ 7.43304560e-01,  1.14182732e+00, -4.92080543e-01, ...,\n          -4.47690448e-01, -1.14681264e+00, -1.76119099e+00],\n         [ 8.97173481e-01, -2.18381137e-01,  3.26755866e-01, ...,\n          -3.90061638e-01, -3.28730603e-01, -6.75031704e-01],\n         ...,\n         [-1.45221334e+00, -2.31124429e-01, -3.91679373e-01, ...,\n          -4.33385322e-02,  4.09008991e-02,  1.35063757e+00],\n         [-5.66902420e-02, -1.73468187e-01,  7.47005753e-01, ...,\n           8.33499962e-01, -1.28903776e+00,  2.35890180e-01],\n         [-1.85688758e+00,  1.72700610e-01,  1.35948581e+00, ...,\n          -3.97882443e-01, -6.05122139e-01,  6.38354956e-01]],\n\n        [[ 1.48523698e+00,  1.00707247e+00, -1.36724552e+00, ...,\n          -3.41441221e-01, -2.41808594e+00,  5.21275162e-01],\n         [-6.22408330e-01, -5.23530133e-01,  8.19780368e-01, ...,\n          -9.02380286e-01,  1.83662147e-01, -9.09107137e-01],\n         [-1.40333339e-01, -6.67659331e-01, -9.16907220e-01, ...,\n           1.14042228e+00,  1.67071661e-01,  5.00211206e-01],\n         ...,\n         [ 3.79914542e-01, -2.39140943e-01,  1.46804170e+00, ...,\n           5.87452506e-01, -3.01500674e-01,  6.02887291e-01],\n         [-1.70329831e-01,  2.79671989e-01, -1.11501175e+00, ...,\n           7.07113866e-01,  1.80408896e+00,  1.38572293e+00],\n         [-8.50432151e-02,  6.28056654e-01,  2.00971386e-01, ...,\n           2.12518498e-01, -8.16341890e-02, -1.10496093e+00]],\n\n        [[ 1.33164710e+00,  7.29017530e-01, -5.80753895e-02, ...,\n          -1.63994389e-01, -1.92245132e-01,  1.32143722e+00],\n         [ 6.73942872e-01,  5.56870225e-01,  9.21593702e-01, ...,\n           3.27008528e-01,  7.92487555e-01, -1.60787996e+00],\n         [ 1.33513795e+00, -1.20409523e+00, -2.99971867e-01, ...,\n           3.11765667e-01,  6.58130565e-01, -7.30112057e-01],\n         ...,\n         [ 2.07758360e+00, -5.00598489e-01,  5.14576020e-01, ...,\n          -1.69679993e+00, -2.48096148e+00,  1.60557038e+00],\n         [-7.16297053e-01,  2.23556843e-01, -9.13124550e-01, ...,\n          -5.17077937e-01, -8.15471527e-01,  2.37690024e-01],\n         [-1.61911479e+00,  9.72540477e-01, -1.80803305e+00, ...,\n          -1.34844911e-01, -1.50651291e+00,  7.77208597e-01]]],\n\n\n       [[[ 7.33478368e-01,  8.09487767e-01,  9.31647966e-01, ...,\n           1.26436694e-01, -8.33928262e-01,  4.14856852e-01],\n         [-1.25220538e+00,  1.32293582e+00,  1.83065114e+00, ...,\n           6.82111463e-01, -1.01890301e+00,  1.40057791e+00],\n         [ 3.57237927e-01, -7.26067859e-01, -9.11814689e-01, ...,\n           6.03604246e-01,  2.60371343e-01,  3.29520296e-01],\n         ...,\n         [-5.55198403e-01, -1.79239758e+00,  3.65536191e-01, ...,\n          -4.91704120e-03, -5.92020244e-01,  1.96485016e+00],\n         [-1.75384265e-02,  3.84366423e-01,  1.37345307e+00, ...,\n           3.96554985e-02, -9.36818006e-01,  4.34851749e-01],\n         [-1.76587791e+00,  7.18612911e-01, -8.38940557e-01, ...,\n          -7.53720487e-01,  4.45974514e-01, -1.67514447e-01]],\n\n        [[-1.02096104e+00,  2.10850645e+00, -1.84531909e+00, ...,\n          -1.28800260e+00, -1.04997392e-01, -9.03265966e-02],\n         [ 5.01764441e-01, -2.50761568e-01, -1.04805856e+00, ...,\n           4.00936392e-01, -7.07239281e-01,  1.28560638e+00],\n         [-7.04865356e-01, -3.61470574e-02,  1.64478064e+00, ...,\n           1.87922594e+00,  5.97782345e-01, -6.71341491e-01],\n         ...,\n         [-4.21010986e-01,  1.51203863e-01, -1.42766523e+00, ...,\n           5.71706519e-01, -2.63302863e-01,  1.15401399e+00],\n         [ 1.48280034e-01,  1.48615005e-01, -7.35821280e-01, ...,\n           1.28692520e+00, -7.11724841e-01,  1.93137853e-01],\n         [-1.45520248e+00, -7.84250202e-01,  3.25253346e+00, ...,\n          -9.69939170e-02, -3.63979911e-01, -1.40679209e-01]],\n\n        [[-1.53265316e+00, -1.53207362e+00, -1.16134805e+00, ...,\n           1.27495709e+00,  9.88227176e-01, -3.47402286e-01],\n         [ 2.82792374e-01,  3.57261698e-01,  1.02204381e+00, ...,\n          -7.91439073e-01,  1.13959194e+00, -1.24279568e+00],\n         [-9.51439181e-01, -1.57826042e-01,  1.25355737e-01, ...,\n           8.96493697e-01, -3.23472924e+00,  1.97201723e+00],\n         ...,\n         [-2.03624642e+00, -2.99824153e-01,  2.59179047e-01, ...,\n          -1.36439894e+00,  2.07855267e+00,  3.75483140e-01],\n         [-1.24157226e-01, -3.45866472e-01,  4.99900698e-01, ...,\n           2.45408594e+00, -7.07974982e-01,  2.03020075e+00],\n         [ 7.37380422e-01, -4.38221370e-01,  9.68364773e-01, ...,\n          -2.65450364e+00,  1.31249933e-01,  5.01701762e-01]]]])>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m model \u001b[39m=\u001b[39m MLPLinearRegressor()\n\u001b[0;32m     54\u001b[0m model\u001b[39m.\u001b[39minitialize(X_train, y_train)\n\u001b[1;32m---> 55\u001b[0m model\u001b[39m.\u001b[39;49mtrain(y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[87], line 46\u001b[0m, in \u001b[0;36mMLPLinearRegressor.train\u001b[1;34m(self, y, epochs)\u001b[0m\n\u001b[0;32m     43\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss()\n\u001b[0;32m     45\u001b[0m     grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvars)\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mapply_gradients(\u001b[39mzip\u001b[39;49m(grads, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvars))\n\u001b[0;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorrects \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mequal(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicted, y)\n\u001b[0;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccuracy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39mcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorrects, tf\u001b[39m.\u001b[39mfloat32))\n",
      "File \u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:689\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_gradients\u001b[39m(\n\u001b[0;32m    649\u001b[0m     \u001b[39mself\u001b[39m, grads_and_vars, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, experimental_aggregate_gradients\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    650\u001b[0m ):\n\u001b[0;32m    651\u001b[0m     \u001b[39m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \n\u001b[0;32m    653\u001b[0m \u001b[39m    This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39m      RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m optimizer_utils\u001b[39m.\u001b[39;49mfilter_empty_gradients(grads_and_vars)\n\u001b[0;32m    690\u001b[0m     var_list \u001b[39m=\u001b[39m [v \u001b[39mfor\u001b[39;00m (_, v) \u001b[39min\u001b[39;00m grads_and_vars]\n\u001b[0;32m    692\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name):\n\u001b[0;32m    693\u001b[0m         \u001b[39m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\utils.py:77\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filtered:\n\u001b[0;32m     76\u001b[0m     variable \u001b[39m=\u001b[39m ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable: \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided `grads_and_vars` is \u001b[39m\u001b[39m{\u001b[39;00mgrads_and_vars\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m     )\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m vars_with_empty_grads:\n\u001b[0;32m     82\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m     83\u001b[0m         (\n\u001b[0;32m     84\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mGradients do not exist for variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m when minimizing the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m         ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vars_with_empty_grads]),\n\u001b[0;32m     89\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: (['f_weights:0', 'h_weights:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'f_weights:0' shape=(3, 1032, 8) dtype=float64, numpy=\narray([[[ 0.50597862, -0.70111141,  0.45309454, ...,  0.57622276,\n         -0.16532128,  0.14021994],\n        [-1.33475533, -0.07734598,  1.43766396, ..., -0.0521582 ,\n          1.07995859, -0.88625755],\n        [-1.02305545,  1.54611159,  1.28781231, ...,  0.14079827,\n         -0.33146609,  1.59719274],\n        ...,\n        [ 0.73296047,  0.0603372 , -2.22900904, ...,  0.34560146,\n         -0.20135526,  0.16414816],\n        [ 1.53523747, -0.54952954, -1.4167278 , ..., -0.38099626,\n          1.34786757, -1.55258931],\n        [ 3.05293555,  0.89496054,  1.0255621 , ...,  1.33858007,\n          0.10878851,  1.1138124 ]],\n\n       [[ 1.72520435, -0.02254615,  0.4565958 , ...,  0.25351815,\n         -1.49348338, -0.02426565],\n        [-1.73683138, -3.13296541, -1.0009193 , ..., -0.6623713 ,\n         -1.55629119, -1.27437937],\n        [-0.52004223, -0.65473797,  0.1437589 , ..., -0.65674196,\n          0.26696839, -0.36011948],\n        ...,\n        [-1.17803299,  0.7581498 ,  2.57487294, ..., -1.42498255,\n         -1.35024898,  0.2043445 ],\n        [-1.92464358,  0.75313672,  0.91514881, ..., -1.02062905,\n          0.06891263,  0.02138806],\n        [-3.33474558,  2.78310279,  0.78618056, ..., -0.4866121 ,\n          0.42650806,  1.65580502]],\n\n       [[ 0.45416331, -1.62202166, -1.0840212 , ...,  0.94933645,\n          0.0423798 ,  0.20728952],\n        [-0.80985414,  1.75969839, -0.63845318, ..., -0.27194673,\n         -1.12112559,  0.13007384],\n        [-2.38110683, -0.11047422, -1.45644089, ..., -0.27660387,\n         -1.38421801,  1.20912123],\n        ...,\n        [ 0.12850507, -0.3466529 , -0.93076991, ...,  0.5410156 ,\n         -0.96472201, -0.41431831],\n        [-0.02140139, -0.96099732, -0.17851672, ..., -0.4628775 ,\n          0.83304263,  0.12199759],\n        [ 0.23580939, -1.34881401,  0.40596882, ...,  0.03102438,\n         -1.61135956,  1.3708435 ]]])>), (None, <tf.Variable 'h_weights:0' shape=(3, 3, 1032, 1032) dtype=float64, numpy=\narray([[[[ 5.05978617e-01, -7.01111411e-01,  4.53094537e-01, ...,\n           2.01663704e-01, -6.87309775e-01,  1.12341927e+00],\n         [-3.94356741e-01,  3.97808510e-01, -1.41928089e+00, ...,\n           3.19940172e-01, -4.66404894e-01, -3.62271970e-01],\n         [ 4.53668614e-02,  1.07113242e+00,  1.06879203e+00, ...,\n           1.26675523e-01,  1.20327052e+00,  1.67265897e+00],\n         ...,\n         [-1.78911155e+00,  1.36082847e+00, -1.88817905e-01, ...,\n          -1.95171538e+00, -7.79169681e-02,  3.33521615e-01],\n         [-3.03668185e-01,  3.72255576e-01, -5.04441293e-01, ...,\n           4.10655743e-01,  1.02266630e+00, -3.36224293e-01],\n         [-7.19580875e-01, -2.45929627e-02, -4.61183365e-01, ...,\n          -1.35409845e+00,  1.11598508e+00, -1.08919482e+00]],\n\n        [[-9.67877317e-01,  2.52314536e-01,  3.42320441e-01, ...,\n           1.07742695e+00, -7.68895277e-02,  1.79941101e+00],\n         [ 4.94328881e-01,  5.12484565e-03,  9.10818098e-01, ...,\n          -1.45903754e+00, -1.67448395e+00,  1.75479179e+00],\n         [ 1.21336286e+00, -1.72999873e-01, -1.16477214e+00, ...,\n           1.85513232e+00, -1.79540337e-03, -3.81298381e-01],\n         ...,\n         [-1.36683062e-01,  1.74906848e+00, -7.95266196e-01, ...,\n          -7.73560432e-01, -2.06929636e-01, -2.30326988e-01],\n         [-6.06685361e-01,  5.04122289e-01, -3.40230956e-01, ...,\n           1.68160856e+00, -9.37543630e-01, -8.85798498e-01],\n         [-1.62052470e-02, -9.48159899e-01,  1.85482769e+00, ...,\n          -2.69687453e-02, -5.77886718e-01,  9.75548857e-01]],\n\n        [[ 7.41917662e-02,  8.07308483e-01, -1.32015877e-01, ...,\n          -2.63321551e+00, -1.55754927e+00, -1.12279820e+00],\n         [ 1.74868012e+00, -1.57442408e+00, -4.05335610e-01, ...,\n           7.37842015e-01,  9.73400726e-01, -6.07860281e-02],\n         [ 1.00761411e-01,  9.56840226e-01,  9.78929271e-01, ...,\n          -7.86378347e-01,  2.35566413e-01,  4.85067309e-01],\n         ...,\n         [ 7.29662501e-01,  1.04465823e-01, -5.17342432e-01, ...,\n           4.68843748e-01, -2.02585139e+00, -8.50086319e-01],\n         [ 1.00846286e+00, -8.70257417e-01,  6.53197863e-01, ...,\n          -5.42855965e-01, -6.79427592e-01, -3.28299837e-01],\n         [ 1.73219711e-01, -1.95214891e+00,  8.06227223e-01, ...,\n           3.31526358e-01,  4.82596937e-01, -8.88833590e-01]]],\n\n\n       [[[ 3.60229238e-01,  3.10395345e+00,  4.89947923e-01, ...,\n           1.04423519e-01,  2.64872555e+00,  1.48999485e+00],\n         [ 7.43304560e-01,  1.14182732e+00, -4.92080543e-01, ...,\n          -4.47690448e-01, -1.14681264e+00, -1.76119099e+00],\n         [ 8.97173481e-01, -2.18381137e-01,  3.26755866e-01, ...,\n          -3.90061638e-01, -3.28730603e-01, -6.75031704e-01],\n         ...,\n         [-1.45221334e+00, -2.31124429e-01, -3.91679373e-01, ...,\n          -4.33385322e-02,  4.09008991e-02,  1.35063757e+00],\n         [-5.66902420e-02, -1.73468187e-01,  7.47005753e-01, ...,\n           8.33499962e-01, -1.28903776e+00,  2.35890180e-01],\n         [-1.85688758e+00,  1.72700610e-01,  1.35948581e+00, ...,\n          -3.97882443e-01, -6.05122139e-01,  6.38354956e-01]],\n\n        [[ 1.48523698e+00,  1.00707247e+00, -1.36724552e+00, ...,\n          -3.41441221e-01, -2.41808594e+00,  5.21275162e-01],\n         [-6.22408330e-01, -5.23530133e-01,  8.19780368e-01, ...,\n          -9.02380286e-01,  1.83662147e-01, -9.09107137e-01],\n         [-1.40333339e-01, -6.67659331e-01, -9.16907220e-01, ...,\n           1.14042228e+00,  1.67071661e-01,  5.00211206e-01],\n         ...,\n         [ 3.79914542e-01, -2.39140943e-01,  1.46804170e+00, ...,\n           5.87452506e-01, -3.01500674e-01,  6.02887291e-01],\n         [-1.70329831e-01,  2.79671989e-01, -1.11501175e+00, ...,\n           7.07113866e-01,  1.80408896e+00,  1.38572293e+00],\n         [-8.50432151e-02,  6.28056654e-01,  2.00971386e-01, ...,\n           2.12518498e-01, -8.16341890e-02, -1.10496093e+00]],\n\n        [[ 1.33164710e+00,  7.29017530e-01, -5.80753895e-02, ...,\n          -1.63994389e-01, -1.92245132e-01,  1.32143722e+00],\n         [ 6.73942872e-01,  5.56870225e-01,  9.21593702e-01, ...,\n           3.27008528e-01,  7.92487555e-01, -1.60787996e+00],\n         [ 1.33513795e+00, -1.20409523e+00, -2.99971867e-01, ...,\n           3.11765667e-01,  6.58130565e-01, -7.30112057e-01],\n         ...,\n         [ 2.07758360e+00, -5.00598489e-01,  5.14576020e-01, ...,\n          -1.69679993e+00, -2.48096148e+00,  1.60557038e+00],\n         [-7.16297053e-01,  2.23556843e-01, -9.13124550e-01, ...,\n          -5.17077937e-01, -8.15471527e-01,  2.37690024e-01],\n         [-1.61911479e+00,  9.72540477e-01, -1.80803305e+00, ...,\n          -1.34844911e-01, -1.50651291e+00,  7.77208597e-01]]],\n\n\n       [[[ 7.33478368e-01,  8.09487767e-01,  9.31647966e-01, ...,\n           1.26436694e-01, -8.33928262e-01,  4.14856852e-01],\n         [-1.25220538e+00,  1.32293582e+00,  1.83065114e+00, ...,\n           6.82111463e-01, -1.01890301e+00,  1.40057791e+00],\n         [ 3.57237927e-01, -7.26067859e-01, -9.11814689e-01, ...,\n           6.03604246e-01,  2.60371343e-01,  3.29520296e-01],\n         ...,\n         [-5.55198403e-01, -1.79239758e+00,  3.65536191e-01, ...,\n          -4.91704120e-03, -5.92020244e-01,  1.96485016e+00],\n         [-1.75384265e-02,  3.84366423e-01,  1.37345307e+00, ...,\n           3.96554985e-02, -9.36818006e-01,  4.34851749e-01],\n         [-1.76587791e+00,  7.18612911e-01, -8.38940557e-01, ...,\n          -7.53720487e-01,  4.45974514e-01, -1.67514447e-01]],\n\n        [[-1.02096104e+00,  2.10850645e+00, -1.84531909e+00, ...,\n          -1.28800260e+00, -1.04997392e-01, -9.03265966e-02],\n         [ 5.01764441e-01, -2.50761568e-01, -1.04805856e+00, ...,\n           4.00936392e-01, -7.07239281e-01,  1.28560638e+00],\n         [-7.04865356e-01, -3.61470574e-02,  1.64478064e+00, ...,\n           1.87922594e+00,  5.97782345e-01, -6.71341491e-01],\n         ...,\n         [-4.21010986e-01,  1.51203863e-01, -1.42766523e+00, ...,\n           5.71706519e-01, -2.63302863e-01,  1.15401399e+00],\n         [ 1.48280034e-01,  1.48615005e-01, -7.35821280e-01, ...,\n           1.28692520e+00, -7.11724841e-01,  1.93137853e-01],\n         [-1.45520248e+00, -7.84250202e-01,  3.25253346e+00, ...,\n          -9.69939170e-02, -3.63979911e-01, -1.40679209e-01]],\n\n        [[-1.53265316e+00, -1.53207362e+00, -1.16134805e+00, ...,\n           1.27495709e+00,  9.88227176e-01, -3.47402286e-01],\n         [ 2.82792374e-01,  3.57261698e-01,  1.02204381e+00, ...,\n          -7.91439073e-01,  1.13959194e+00, -1.24279568e+00],\n         [-9.51439181e-01, -1.57826042e-01,  1.25355737e-01, ...,\n           8.96493697e-01, -3.23472924e+00,  1.97201723e+00],\n         ...,\n         [-2.03624642e+00, -2.99824153e-01,  2.59179047e-01, ...,\n          -1.36439894e+00,  2.07855267e+00,  3.75483140e-01],\n         [-1.24157226e-01, -3.45866472e-01,  4.99900698e-01, ...,\n           2.45408594e+00, -7.07974982e-01,  2.03020075e+00],\n         [ 7.37380422e-01, -4.38221370e-01,  9.68364773e-01, ...,\n          -2.65450364e+00,  1.31249933e-01,  5.01701762e-01]]]])>))."
     ]
    }
   ],
   "source": [
    "class MLPLinearRegressor(object):\n",
    "    def __init__(self, lr=0.001, n_layers=3, n_neurons=3):\n",
    "        self.graph = tf.Graph()\n",
    "        self.lr = 0.001\n",
    "        self.n_layers = n_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.initialized = False\n",
    "\n",
    "    @tf.function\n",
    "    def loss(self):\n",
    "        return 1/2 * tf.reduce_sum( tf.square((tf.reshape(self.predicted, [-1, 1]) - self.y)))\n",
    "\n",
    "    def initialize(self, X, y):\n",
    "          n_samples, n_features = X.shape\n",
    "          n_output = y.shape[0]\n",
    "          normal_initiliazer = tf.random_normal_initializer(seed=0, mean=0.0, stddev=1.0)\n",
    "          XN = tf.keras.utils.normalize(X)\n",
    "          self.X = tf.constant(XN, name=\"X\", dtype=tf.double)\n",
    "          self.y = tf.constant(np.array(y).reshape(-1, 1), name=\"y\", dtype=tf.double)\n",
    "\n",
    "          self.f_weights = tf.Variable(normal_initiliazer([self.n_neurons, n_samples, n_features], dtype=tf.double), name=\"f_weights\", dtype=tf.double, trainable=True )\n",
    "          self.h_weights = tf.Variable(normal_initiliazer([self.n_layers, self.n_neurons, n_samples, n_output], dtype=tf.double), name=\"h_weights\", dtype=tf.double, trainable=True )\n",
    "          self.bias = tf.Variable([0.01 for l in range(self.n_layers)], name=\"bias\", dtype=tf.double, trainable=True )\n",
    "          \n",
    "          self.activation = tf.TensorArray(tf.double, size=0, dynamic_size=True, clear_after_read=False, tensor_array_name=\"Activation_Layers\")\n",
    "          self.activation.write(0, [tf.math.sigmoid(tf.matmul(X, tf.transpose(self.f_weights[n])) + self.bias[0]) for n in range(self.n_neurons)]).mark_used()\n",
    "          for l in range(1,self.n_layers-1):\n",
    "              self.activation.write(l, [tf.math.sigmoid(tf.matmul(self.activation.read(l-1)[n], self.h_weights[l][n]) + self.bias[l]) for n in range(self.n_neurons)])\n",
    "          self.activation.write(self.n_layers-1, [tf.math.sigmoid(tf.matmul(self.activation.read(self.n_layers-2)[n], self.h_weights[self.n_layers-1][n]) + self.bias[self.n_layers-1]) for n in range(self.n_neurons)])\n",
    "\n",
    "          self.predicted = tf.Variable(tf.reduce_mean(tf.math.sigmoid(tf.tensordot(tf.reduce_mean(tf.reduce_mean(self.h_weights[self.n_layers-1], axis=0), axis=0), tf.reduce_mean(tf.reduce_mean(self.activation.read(self.n_layers-1), axis=0), axis=1), axes=0) + tf.reshape(self.bias[self.n_layers-1], [-1, 1])), axis=0), trainable=True, name=\"Predicted\")\n",
    "          #self.loss = lambda : tf.losses.mean_squared_error(self.predicted, y)\n",
    "          self.learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(self.lr, 5, 0.85, name=\"Learning_Rate\")\n",
    "          self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "\n",
    "          self.vars = [self.f_weights, self.h_weights, self.bias, self.predicted]\n",
    "          self.initialized = True\n",
    "\n",
    "    def train(self, y, epochs=75):\n",
    "        if self.initialized:\n",
    "          for e in range(epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = self.loss()\n",
    "            \n",
    "            grads = tape.gradient(loss, self.vars)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.vars))\n",
    "        self.corrects = tf.equal(self.predicted, y)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.corrects, tf.float32))\n",
    "\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.95)\n",
    "\n",
    "model = MLPLinearRegressor()\n",
    "model.initialize(X_train, y_train)\n",
    "model.train(y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cce27c8212bf6c5aa96f33a3d1153887721b66c5c8cb9adeaa83cce09196b75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
