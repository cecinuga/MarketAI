{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, <tf.Tensor: shape=(1032,), dtype=float64, numpy=\n",
      "array([-0.57045794, -2.77345856, -0.09445794, ..., -0.70745783,\n",
      "       -1.5974579 , -3.05045767])>]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['f_weights:0', 'h_weights:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "class MLPLinearRegressor(object):\n",
    "    def __init__(self, lr=0.001, n_layers=3, n_neurons=3):\n",
    "        self.graph = tf.Graph()\n",
    "        self.lr = 0.001\n",
    "        self.n_layers = n_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.initialized = False\n",
    "\n",
    "    def loss(self):\n",
    "        return 1/2 * tf.reduce_sum( tf.square((tf.reshape(self.predicted, [-1, 1]) - self.y)))\n",
    "\n",
    "    def initialize(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_output = y.shape[0]\n",
    "        normal_initiliazer = tf.random_normal_initializer(seed=0, mean=0.0, stddev=1.0)\n",
    "        XN = tf.keras.utils.normalize(X)\n",
    "        self.X = tf.constant(XN, name=\"X\", dtype=tf.double)\n",
    "        self.y = tf.constant(np.array(y).reshape(-1, 1), name=\"y\", dtype=tf.double)\n",
    "\n",
    "        self.f_weights = tf.Variable(normal_initiliazer([self.n_neurons, n_samples, n_features], dtype=tf.double), name=\"f_weights\", dtype=tf.double, trainable=True )\n",
    "        self.h_weights = tf.Variable(normal_initiliazer([self.n_layers, self.n_neurons, n_samples, n_output], dtype=tf.double), name=\"h_weights\", dtype=tf.double, trainable=True )\n",
    "        self.bias = tf.Variable([0.01 for l in range(self.n_layers)], name=\"bias\", dtype=tf.double, trainable=True )\n",
    "        \n",
    "        self.activation = tf.TensorArray(tf.double, size=0, dynamic_size=True, clear_after_read=False, tensor_array_name=\"Activation_Layers\")\n",
    "        self.activation.write(0, [tf.math.sigmoid(tf.matmul(X, tf.transpose(self.f_weights[n])) + self.bias[0]) for n in range(self.n_neurons)]).mark_used()\n",
    "        for l in range(1,self.n_layers-1):\n",
    "            self.activation.write(l, [tf.math.sigmoid(tf.matmul(self.activation.read(l-1)[n], self.h_weights[l][n]) + self.bias[l]) for n in range(self.n_neurons)]).mark_used()\n",
    "        self.activation.write(self.n_layers-1, [tf.math.sigmoid(tf.matmul(self.activation.read(self.n_layers-2)[n], self.h_weights[self.n_layers-1][n]) + self.bias[self.n_layers-1]) for n in range(self.n_neurons)]).mark_used()\n",
    "\n",
    "        #self.loss = lambda : tf.losses.mean_squared_error(self.predicted, y)\n",
    "        self.learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(self.lr, 5, 0.85, name=\"Learning_Rate\")\n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "        self.predicted = tf.Variable(tf.reduce_mean(tf.math.sigmoid(tf.tensordot(tf.reduce_mean(tf.reduce_mean(self.h_weights[self.n_layers-1], axis=0), axis=0), tf.reduce_mean(tf.reduce_mean(self.activation.read(self.n_layers-1), axis=0), axis=1), axes=0) + tf.reshape(self.bias[self.n_layers-1], [-1, 1])), axis=0), trainable=True,dtype=tf.double, name=\"Predicted\")\n",
    "\n",
    "        self.vars = [self.f_weights, self.h_weights, self.predicted]\n",
    "        self.initialized = True\n",
    "\n",
    "    def train(self, y, epochs=75):\n",
    "        if self.initialized:\n",
    "          for e in range(epochs):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                tape.watch(self.vars)\n",
    "                loss = self.loss()\n",
    "            \n",
    "            grads = tape.gradient(loss, self.vars)\n",
    "            print(grads)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.vars))\n",
    "        #self.accuracy = tf.keras.metrics.Accuracy().update_state(self.y, self.predicted)\n",
    "        self.error = tf.keras.metrics.MeanSquaredError().update_state(tf.reshape(tf.sigmoid(self.y), [1, -1]), self.predicted)\n",
    "        print(self.error.numpy())\n",
    "          \n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.95)\n",
    "\n",
    "model = MLPLinearRegressor()\n",
    "model.initialize(X_train, y_train)\n",
    "model.train(y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cce27c8212bf6c5aa96f33a3d1153887721b66c5c8cb9adeaa83cce09196b75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
