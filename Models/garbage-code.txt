"""if not self.build:
            n_samples, n_features = self.X.shape
            n_output = self.y.shape[0]
            normal_initiliazer = tf.random_normal_initializer(seed=0, mean=0.0, stddev=1.0)
            XN = tf.keras.utils.normalize(self.X)
            self.X = tf.constant(XN, name="X", dtype=tf.double)
            self.y = tf.reshape(self.y, [-1, 1])
            self.f_weights = tf.Variable(normal_initiliazer([self.n_neurons, n_samples, n_features], dtype=tf.double), name="f_weights", dtype=tf.double, trainable=True )
            self.h_weights = tf.Variable(normal_initiliazer([self.n_layers, self.n_neurons, n_samples, n_output], dtype=tf.double), name="h_weights", dtype=tf.double, trainable=True )
            self.bias = tf.Variable([0.01 for l in range(self.n_layers)], name="bias", dtype=tf.double, trainable=True )
            #self.loss = lambda : tf.losses.mean_squared_error(self.predicted, y)

            
            self.h_weights[0].assign([tf.math.sigmoid(tf.matmul(self.X, tf.transpose(self.f_weights[n])) + self.bias[0]) for n in range(self.n_neurons)])
            for l in range(1,self.n_layers-1):
                self.h_weights[l].assign([tf.math.sigmoid(tf.matmul(self.h_weights[l-1][n], self.h_weights[l][n]) + self.bias[l]) for n in range(self.n_neurons)])
            self.h_weights[self.n_layers-1].assign([tf.math.sigmoid(tf.matmul(self.h_weights[self.n_layers-2][n], self.h_weights[self.n_layers-1][n]) + self.bias[self.n_layers-1]) for n in range(self.n_neurons)])
            self.build = True


            self.h_weights[0].assign([tf.math.sigmoid(tf.matmul(self.X, tf.transpose(self.f_weights[n])) + self.bias[0]) for n in range(self.n_neurons)])
            for l in range(1,self.n_layers-1):
                self.h_weights[l].assign([tf.math.sigmoid(tf.matmul(self.h_weights[l-1][n], self.h_weights[l][n]) + self.bias[l]) for n in range(self.n_neurons)])
            self.h_weights[self.n_layers-1].assign([tf.math.sigmoid(tf.matmul(self.h_weights[self.n_layers-2][n], self.h_weights[self.n_layers-1][n]) + self.bias[self.n_layers-1]) for n in range(self.n_neurons)])
        


    @tf.function    
    def init(self):
        if not self.build:
            self.learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(self.lr, 5, 0.85, name="Learning_Rate")
            self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)

            self.vars = [self.f_weights, self.h_weights]
            self.build = True
        
            return tf.reduce_mean(tf.math.sigmoid(tf.tensordot(tf.reduce_mean(tf.reduce_mean(self.h_weights[self.n_layers-1], axis=0), axis=0), tf.reduce_mean(tf.reduce_mean(self.h_weights[self.n_layers-1], axis=0), axis=1), axes=0) + tf.reshape(self.bias[self.n_layers-1], [-1, 1])), axis=0)        
    
    @tf.function
    
    def train(self, epochs=75):  
        for e in range(epochs):
            with tf.GradientTape(persistent=False, watch_accessed_variables=True) as tape:
                predicted = self.init()
                loss = self.loss(predicted)

            grads = tape.gradient(loss, self.vars)
            self.optimizer.apply_gradients(zip(grads, self.vars))

        self.error =
        #self.error = tf.keras.metrics.MeanSquaredError().update_state(tf.reshape(tf.sigmoid(self.y), [1, -1]), predicted)


        class Layer(tf.Module):
    def __init__(self, s_weights, n_neurons=3, activation=tf.identity):
        self.n_neurons = n_neurons
        self.s_weights = s_weights
        self.activation = activation
        self.build = False
        self.normalized = False
    
    @tf.function(reduce_retracing=True)
    def xavier_init(self, shape):
        in_dim, out_dim = shape
        xavier_lim = tf.sqrt(6.)/tf.sqrt(tf.cast(in_dim + out_dim, tf.float32))
        weight_vals = tf.cast(tf.random.uniform(shape=(in_dim, out_dim), 
                                        minval=-xavier_lim, maxval=xavier_lim, seed=22, dtype=tf.float32), dtype=tf.double)
        return weight_vals

    def __call__(self, X, y):
        n_samples, n_features = X.shape
        n_output = y.shape[0]
        self.weights = tf.Variable(self.xavier_init(shape=(n_features, self.s_weights)), name="Weights", dtype=tf.double, trainable=True )
        self.bias = tf.Variable(tf.zeros(shape=self.s_weights, dtype=tf.double), name="Bias", dtype=tf.double, trainable=True )
        z = tf.add(tf.matmul(X, self.weights), self.bias)
        print(X.shape,"*",self.weights.shape,"+",self.bias.shape)
        return self.activation(z)

    

class MLPLinearRegressor(tf.Module):
    def __init__(self, X, y, layers, lr=0.001):
        self.X = X
        self.y = y
        self.layers = layers
        self.optimizer = tf.keras.optimizers.Adam(lr)
        self.accuracy = tf.keras.metrics.Accuracy()

    @tf.function(reduce_retracing=True)
    def accuracy_rrsse(self, predicted):
        return tf.divide(tf.sqrt(
            tf.divide(
                tf.reduce_sum(tf.square(tf.abs((tf.subtract(predicted, tf.sigmoid(self.y)))))),
                tf.reduce_sum(tf.square(tf.abs((tf.subtract(tf.reduce_mean(tf.sigmoid(self.y)), predicted)))))
            )), len(predicted)
        )

    def normalize_dataset(self, X):
        return tf.convert_to_tensor(tf.keras.utils.normalize(X))


    @tf.function(reduce_retracing=True)
    def loss(self, predicted):
        #return 1/2 * tf.reduce_sum( tf.square(tf.subtract(tf.sigmoid(self.y), tf.squeeze(predicted))))
        return tf.losses.mean_squared_error(self.y, predicted)

    def calc_metrics(self, y_pred):
        self.acc_rrsse = self.accuracy_rrsse(y_pred)

    def train_step(self, X, y):
        for layer in self.layers:
            X = layer(X, y)
            print(X)
        return X
           
    def train(self, epochs=1, lr=0.001):
        for e in range(epochs):
            with tf.GradientTape(watch_accessed_variables=True, persistent=True) as tape:
                self.y_pred = tf.squeeze(model.train_step(self.normalize_dataset(self.X), self.y))
                loss = self.loss(self.y_pred)
            
            self.calc_metrics(self.y_pred)
            self.accuracy.update_state(self.y, self.y_pred)

            self.vars = [self.layers[0].weights, self.layers[1].weights, self.layers[2].weights, self.layers[0].bias, self.layers[1].bias, self.layers[2].bias]

            grads = tape.gradient(loss, self.vars) 
            self.optimizer.apply_gradients(zip(grads, self.vars))


X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.95, random_state=2)

model = MLPLinearRegressor(X_train, y_train, [
    Layer(50,activation=tf.nn.relu),
    Layer(50,activation=tf.nn.relu),
    Layer(1,activation=tf.sigmoid)
])
model.train(epochs=1, lr=0.1)
print('predicted:', model.y_pred.numpy())
print('y:', tf.sigmoid(y_train).numpy())
print("Accuracy: ", model.acc_rrsse.numpy())
print("Keras Accuracy: ", model.accuracy.result().numpy())
"""